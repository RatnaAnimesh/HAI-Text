# Hierarchical Active Inference (HAI) for Text Generation

This repository contains a PyTorch implementation of a **Hierarchical Active Inference (HAI)** agent designed for structured text generation. The project serves as a proof-of-concept demonstrating that a generative model built on neuroscientific first principles‚Äînamely the Free Energy Principle‚Äîcan learn complex sequences like Shakespearean English.

The core thesis is that a model with a factorized, structured understanding of its environment (e.g., separating "what" is being said from "how" it's said) can achieve robust and data-efficient learning.

## üß† Core Concepts

This model is not a standard LLM. It's an agent that builds an internal model of its world (the text) and continuously refines it by trying to minimize its own "surprise" or **Variational Free Energy (VFE)**.

1.  **POMDP (`generative_model.py`)**: The world is modeled as a **P**artially **O**bservable **M**arkov **D**ecision **P**rocess. This means the agent believes the text it observes is generated by hidden (partially observable) states that evolve over time. We define two types of hidden states:
    *   **Semantic States (`n_sem`)**: The abstract meaning or context.
    *   **Syntactic States (`n_syn`)**: The grammatical structure or role.

2.  **Variational Free Energy (`free_energy.py`)**: This is the central objective function for learning. By minimizing VFE, the agent is forced to learn a world model that balances **Accuracy** and **Complexity**.
    *   **Accuracy**: How well do my beliefs explain the characters I'm seeing?
    *   **Complexity**: How much do I have to change my prior beliefs to explain what I'm seeing?

3.  **Active Inference & EFE (`policies.py`)**: The agent doesn't just passively learn; it acts (generates text). It chooses actions by calculating the **Expected Free Energy (EFE)** for future possibilities. This allows it to select actions that are both goal-oriented and information-seeking.
    *   **Pragmatic Value**: "Will this action lead to outcomes I prefer?"
    *   **Epistemic Value (Curiosity)**: "Will this action help me resolve uncertainty about the hidden states of the world?"

4.  **Hierarchy (`hierarchical.py`)**: The model is composed of two stacked POMDPs. A high-level model tracks abstract, slow-moving context, and its beliefs directly influence the transition dynamics of a low-level model that deals with fast-moving characters. This allows the model to learn structure at multiple time scales.

5.  **Tensor-Train Decomposition (`tensor_train.py`)**: To make the massive tensors (like the transition matrix) computationally tractable, this project uses Tensor-Train decomposition to represent them in a highly compressed format.

## ‚öôÔ∏è How It Works: The Learning Cycle

The training process, orchestrated by `HAITrainer`, is an implementation of a **Variational Expectation-Maximization** algorithm. For each batch of text:

1.  **E-Step (Belief Inference)**: The agent is given a sequence of characters. It has no direct access to the "true" hidden states that generated them. It uses `forward_backward_update` (`inference.py`) to infer a posterior distribution (a belief) over these hidden states. This is an iterative process where the agent refines its beliefs until they converge.

2.  **M-Step (Model Update)**: The agent compares its inferred beliefs to its prior predictions using the **Variational Free Energy** objective function (`free_energy.py`). The resulting `loss` value quantifies the agent's "surprise." PyTorch's autograd engine then calculates the gradient of this loss with respect to all model parameters (`A`, `B`, `D` matrices), and the `Adam` optimizer takes a small step to update the parameters, making the agent slightly less surprised in the future.

This cycle repeats, gradually improving the agent's internal world model.

## üöÄ Quick Start

### Prerequisites
*   Python 3.8+
*   PyTorch
*   NumPy
*   Requests

### Installation
```bash
git clone https://github.com/RatnaAnimesh/HAI-Text.git
cd HAI-Text
pip install -r requirements.txt  # Assuming you create a requirements.txt
```
*(Note: A `requirements.txt` file would be ideal. For now, `pip install torch numpy requests` will suffice.)*

### Training a New Model
To train the large-scale model on the Tiny Shakespeare dataset from scratch:

```bash
python hai_text/scripts/train_shakespeare.py
```

This script will:
1.  Download the `input.txt` dataset if not found.
2.  Initialize a `DiscretePOMDP` with 512 semantic and 512 syntactic states.
3.  Instantiate the `HAITrainer`.
4.  Run the training loop, saving model checkpoints to `hai_text/data/`.

### Running the Validation Script
To verify the core mathematical logic of the model on a simple, hand-crafted grammar task ("cat eats fish"):

```bash
python hai_text/scripts/validate_pomdp.py
```
This is an excellent way to understand how the inference and EFE mechanisms work in a controlled environment.

## üìÇ Project Structure

*   `hai_text/`
    *   `data/`: Stores the raw text (`input.txt`), tokenizers, and saved model checkpoints.
        *   `tokenizer.py`: A simple character/word-level tokenizer.
    *   `models/`: Contains the core components of the Active Inference agent.
        *   `generative_model.py`: Defines the `DiscretePOMDP` class (the agent's world model).
        *   `inference.py`: Implements the `forward_backward_update` algorithm for belief inference.
        *   `free_energy.py`: Defines the VFE objective function for learning.
        *   `policies.py`: Implements EFE calculation for action selection (generation).
        *   `hierarchical.py`: Defines the two-level hierarchical model structure.
        *   `tensor_train.py`: Provides tools for Tensor-Train decomposition to keep the model efficient.
    *   `training/`: Contains the code for model training.
        *   `dataset.py`: A PyTorch `Dataset` to prepare sequences for the trainer.
        *   `trainer.py`: The main `HAITrainer` class that orchestrates the E-Step and M-Step of learning.
    *   `scripts/`: High-level scripts for running experiments.
        *   `train_shakespeare.py`: The main script to train the large model on Shakespeare.
        *   `validate_pomdp.py`: A script for sanity-checking the model's logic on a toy grammar.
    *   `tests/`: Contains unit tests for individual components of the model.

## License
This project is licensed under the MIT License.